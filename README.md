# **Text2SQL â€“ Clustering-Enhanced Fine-Tuning Framework**

This repository contains all code and experiments for our **Text-to-SQL fine-tuning project**, where we evaluate multiple training strategies on **Spider 1.0** and **BIRD**, including clustering-based routing methods and classical ML baselines.
The core model used throughout is **CodeLlama**, fine-tuned under multiple configurations.

---

## Demo Link: https://www.dropbox.com/scl/fi/bm259r584kjovl4lwb563/Screen-Recording-2025-12-01-at-10.17.14-AM.mov?rlkey=4rt976tuh2i74sf07je7y1r14&st=7exj0gty&dl=0

## ğŸš€ **Project Overview**

We trained and evaluated the following pipelines:

---

## **1. Standard Fine-Tuning (Baseline)**

### Datasets:

* **Spider 1.0**
* **BIRD**

### Method:

* Standard supervised fine-tuning (SFT)
* Train on `(NL question â†’ SQL)` pairs
* Evaluate exact match (EM), execution accuracy

Scripts:

```
Finetuning/
â”œâ”€â”€ preprocess_spider.py
â”œâ”€â”€ finetune_codellama.py
â”œâ”€â”€ inference_codellama_sql.py
â”œâ”€â”€ generate_predictions.py
â””â”€â”€ evaluate_sql_model.py
```

---

## **2. Clustering-Based Methods (Spider 1.0)**

To improve generalization and reduce semantic drift, we implemented two cluster-based specialization pipelines:

---

# ğŸ§  **A. Natural Language (NL) Text Clustering**

Directory: `NL_text_clustering/`

### **Goal:**

Cluster natural-language questions based on semantic embeddings.

### **Pipeline:**

1. Encode Spider questions using Sentence Transformers.
2. Run **K-Means** to obtain semantic clusters.
3. Split dataset into `train_cluster_k.jsonl` per cluster.
4. Fine-tune CodeLlama **per cluster**.
5. **Routing at inference:** assign test query to nearest cluster centroid.

### **Files:**

```
NL_text_clustering/
â”œâ”€â”€ cluster_spider.py
â”œâ”€â”€ compute_cluster_centroids.py
â”œâ”€â”€ evaluate_sql_model_using_clusters.py
â””â”€â”€ clusters/
    â”œâ”€â”€ cluster_assignments.json
    â”œâ”€â”€ cluster_centroids.npy
    â”œâ”€â”€ cluster_sizes.json
    â”œâ”€â”€ train_cluster_0.jsonl
    â”œâ”€â”€ train_cluster_1.jsonl
    â”œâ”€â”€ train_cluster_2.jsonl
    â””â”€â”€ train_cluster_3.jsonl
```

---

# ğŸŒ³ **B. SQL AST-Based Clustering**

Directory: `SQL_AST_clustering/`

### **Goal:**

Cluster SQL queries by **structural similarity** using ASTs.

### **Pipeline:**

1. Convert each SQL query â†’ AST.
2. Compute tree-based vector representation.
3. Run clustering using structural similarity.
4. Fine-tune one model per AST cluster.
5. At inference, assign to nearest AST cluster.

### **Files:**

```
SQL_AST_clustering/
â”œâ”€â”€ ast_cluster_spider.py
â”œâ”€â”€ cluster_trc.py
â”œâ”€â”€ nltk_load.py
â””â”€â”€ requirements.txt
```

---

## ğŸ” **3. Supervised Cluster Assignment (Routing Classifier)**

After creating clusters (both NL and AST), we additionally trained a **supervised classifier** to predict a queryâ€™s cluster ID automatically.

### Why?

* Nearest-centroid routing is simple but sometimes noisy.
* A classifier trained on embedded questions improves assignment accuracy.

### Classifier types tested:

* Logistic Regression
* Linear SVM
* Random Forest
* Simple 2-layer feed-forward neural network

These models take *query embeddings* as input and predict the cluster label.
This yielded better routing than pure distance-based assignment.

---

## ğŸ§ª **4. ML-Based Baseline â€“ IRNet (for comparison)**

To compare our LLM-based fine-tuning strategies against classical neural semantic parsers, we evaluated:

### âœ” **IRNet (Information-Retrieval Augmented Text-to-SQL Parser)**

A strong neural baseline model used widely for Spider research.

### GitHub Repository (official Microsoft repo):

ğŸ‘‰ **[https://github.com/microsoft/IRNet](https://github.com/microsoft/IRNet)**

We include it as a baseline reference but do not re-train it inside this repo.

---

# ğŸ“‚ **Repository Structure**

```
Text_to_SQL/
â”‚
â”œâ”€â”€ Finetuning/                   # SFT training & evaluation scripts
â”œâ”€â”€ NL_text_clustering/          # Natural language cluster pipeline
â”œâ”€â”€ SQL_AST_clustering/          # SQL AST clustering pipeline
â””â”€â”€ README.md
```

---

# ğŸ“Š **Evaluation**

We evaluate the following:

### **Baselines**

* Standard CodeLlama fine-tuning on Spider & BIRD
* IRNet (external baseline)

### **Clustering Approaches**

* NL Clusters (per-cluster models + ensemble)
* AST Clusters (per-cluster models + ensemble)
* Supervised Cluster Routing (classifier)
* Centroid-Based Routing

### Metrics:

* **Execution Accuracy**: It measures the semantic correctness of a predicted SQL query by executing both the predicted and gold queries on the target database and comparing their returned results. A prediction is counted as correct if the two result sets match exactly, regardless of whether the SQL strings themselves differ syntactically.

---

# ğŸ”§ **Reproduction Steps**

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Preprocess Spider

```bash
python Finetuning/preprocess_spider.py
```

### 3. Run Standard Fine-Tuning

```bash
python Finetuning/finetune_codellama.py \
    --train_file train.json \
    --output_dir outputs/codellama_spider
```

### 4. Run Clustering Pipelines

**Natural Language Clustering**

```bash
python NL_text_clustering/cluster_spider.py
```

**SQL AST Clustering**

```bash
python SQL_AST_clustering/ast_cluster_spider.py
```

### 5. Evaluate

```bash
python Finetuning/evaluate_sql_model.py
python Finetuning/evaluate_sql_model_using_clusters.py
```

---

# ğŸ§© **Key Contributions**

* Fine-tuning CodeLlama on Spider and BIRD
* Natural language semantic clustering (K-Means)
* SQL AST structural clustering
* Routing via both **centroid** and **supervised classifier**
* Comparison with classical IRNet model
* Cluster-specialized model ensemble for improved Text-to-SQL accuracy

---

